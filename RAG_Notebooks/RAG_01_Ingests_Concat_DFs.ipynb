{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"aa7ee434e91640de82c6d32408163b52","deepnote_cell_type":"markdown","id":"_MTnQMGhbWLx"},"source":["**Preparation of SEC Complaint Documents, Enron Annual Reports, and Enron Case Study for RAG**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"7f2966f7e6554217889ab031fb8afec9","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Using Unstructured.io for quick ingestion of the annual report documents. Not as precise as rule-based parsing done in Milestone 1, but quick and easy.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"b45821a08b0b4737926400ca58023ff2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9440,"execution_start":1718209568025,"id":"zeZTcg8ksKtv","source_hash":"52eebbb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["pip install -q unstructured"]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"299f4ed213694d1199a622640c51b343","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":43,"execution_start":1718210264584,"id":"66xPc_C6xeAq","source_hash":"f48e1a9d"},"outputs":[],"source":["import os\n","import pickle\n","\n","from collections import defaultdict\n","\n","import pandas as pd\n","\n","from unstructured.partition.text import partition_text\n"]},{"cell_type":"markdown","metadata":{"cell_id":"215c2e27caae48c4a3950d46b65ebb59","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Process Enron Good Bad Lessons Case Study (which we formatted as a text document in pipe delimited format where section titles are GroupName equivalents)"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"6f78c717507a4cc7a506496d93c36b43","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":201,"execution_start":1718210273945,"source_hash":"63d936f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           GroupName  \\\n","0                                     Enron Overview   \n","1  Why didn’t Arthur Andersen provide adequate as...   \n","2  Could the SEC and other regulators have done m...   \n","3  Why didn’t external financial analysts detect ...   \n","4  Why was Sherron Watkins a lone voice in asking...   \n","5  What changes have occurred or are expected to ...   \n","6                       Enron Timeline: 1988 to 1997   \n","7           The Meteoric Rise Of Enron: 1998 to 2001   \n","8                  A Sudden Collapse July 2001 to ??   \n","\n","                                        grouped_text  \n","0  Enron created 3500–4000 Special Purpose Entiti...  \n","1  Critics note that Arthur Anderson (AA) was too...  \n","2  In 2000, half of Arthur Andersen’s $52 million...  \n","3  How can we detect other companies in similar d...  \n","4  As is common in a firm with a powerful, domine...  \n","5  The Enron scandal itself has already brought s...  \n","6  In 1985, Enron started as Houston Natural Gas....  \n","7  1998 - Enron acquires Wessex Water and enters ...  \n","8  2001 July - New CEO Skilling is hit in the fac...  \n"]}],"source":["file_path = \"/work/data/Enron_Good_Bad.txt\"\n","\n","# Specify header=None to avoid assuming the first line is a header\n","df = pd.read_csv(\n","    file_path,\n","    delimiter=\"|\",\n","    header=None,\n","    names=[\"GroupName\", \"grouped_text\"],\n","    skip_blank_lines=False,\n","    encoding=\"utf-8\",\n","    engine=\"python\",\n",")\n","\n","df[\"GroupName\"] = df[\"GroupName\"]\n","df[\"grouped_text\"] = df[\"grouped_text\"]\n","\n","print(df)"]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"89ec0b8f9d4a43c89a61803e52f676a8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":160,"execution_start":1718210274150,"source_hash":"797da7e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframe saved to pickle file: /work/data/EnronGoodBad.pkl\n"]}],"source":["# Path to output pickle file\n","pickle_path = \"/work/data/EnronGoodBad.pkl\"\n","\n","# Save the dataframe to a pickle so we have this intermediate source in expected column format\n","df.to_pickle(pickle_path)\n","\n","print(f\"Dataframe saved to pickle file: {pickle_path}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"3b15a2f2a497424bb683df9d07a456fd","deepnote_cell_type":"markdown","id":"wdBeBkX08GWQ"},"source":["Ingestion loop for Annual Report (10-K text files) follows. GroupNames created by the Title elements output with Unstructured.io.\n","\n","(Not an exact process - but part of the experiment regarding quick ingestion.)"]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"454cae3be69f4186a3fe7d6db4db220f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":58,"execution_start":1718210274321,"id":"Wx1k1POcxO7X","source_hash":"4596638b"},"outputs":[],"source":["# Used by functions below\n","def parse_elements_to_dataframe(elements):\n","    \"\"\"\n","    This function parses elements based on category and builds a DataFrame with grouped text information.\n","    \"\"\"\n","    current_group_name = None\n","    current_group_text = \"\"\n","    # Create an empty list to store DataFrames\n","    df_list = []\n","\n","    for element in elements:\n","        if element.category == \"Title\":\n","            # Start a new group with the Title text\n","            current_group_name = element.text\n","            current_group_text = \"\"\n","        elif element.category == \"NarrativeText\":\n","            # Append NarrativeText to the current group, adding a space separator if needed\n","            current_group_text += (\" \" if current_group_text else \"\") + element.text\n","        else:\n","            # Ignore elements from other categories\n","            pass\n","\n","        # If a group name is available and there's content, create a new DataFrame and append\n","        if current_group_name and current_group_text:\n","            df_list.append(\n","                pd.DataFrame(\n","                    {\n","                        \"GroupName\": [current_group_name],\n","                        \"grouped_text\": [current_group_text],\n","                    }\n","                )\n","            )\n","            # Reset current variables for next group\n","            current_group_name = None\n","            current_group_text = \"\"\n","\n","    # Concatenate all DataFrames in list into single DataFrame\n","    df = pd.concat(df_list, ignore_index=True)\n","    return df"]},{"cell_type":"code","execution_count":22,"metadata":{"cell_id":"59ab89adf5d543028b9106dff046dfc7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":48,"execution_start":1718210274331,"id":"yB5Xat_s8Ezz","source_hash":"c5453f49"},"outputs":[],"source":["def process_fin_documents(document_paths, output_dir=None):\n","    \"\"\"\n","    This function iterates through document paths, parses each document, and saves the resulting DataFrame as a CSV file.\n","    Args:\n","        document_paths (list): list of document paths.\n","        output_dir (str, optional): directory to save the output CSV files. If not provided,\n","            files are saved in the same directory as the documents. Defaults to None.\n","    \"\"\"\n","    for document_path in document_paths:\n","        # Extract filename without path and extension\n","        filename = os.path.splitext(os.path.basename(document_path))[0]\n","\n","        # Determine output filename based on output_dir\n","        if output_dir:\n","            output_file = os.path.join(output_dir, f\"{filename}_df_group.csv\")\n","        else:\n","            output_file = f\"{filename}_df_group.csv\"\n","\n","        # Partition the document and get the DataFrame\n","        elements = partition_text(filename=document_path)\n","        parsed_df = parse_elements_to_dataframe(elements)\n","\n","        # Save the DataFrame to a CSV file\n","        parsed_df.to_csv(output_file, index=False)\n","\n","        print(f\"Parsed document: {document_path}, saved as: {output_file}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"cell_id":"a45900c7cd8d4dfc9ee9c5e95805f9dc","colab":{"base_uri":"https://localhost:8080/"},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":23272,"execution_start":1718210274352,"id":"MT0cevw08a59","outputId":"e80a7080-a9f7-4cb0-e189-75ce213e959a","source_hash":"c3c27c01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parsed document: /work/data/2000_10k.txt, saved as: /work/data/2000_10k_df_group.csv\n","Parsed document: /work/data/1999_10k.txt, saved as: /work/data/1999_10k_df_group.csv\n","Parsed document: /work/data/1998_10k.txt, saved as: /work/data/1998_10k_df_group.csv\n","Parsed document: /work/data/1997_10k.txt, saved as: /work/data/1997_10k_df_group.csv\n","Parsed document: /work/data/1996_10k.txt, saved as: /work/data/1996_10k_df_group.csv\n","Parsed document: /work/data/1995_10k.txt, saved as: /work/data/1995_10k_df_group.csv\n","Parsed document: /work/data/1994_10k.txt, saved as: /work/data/1994_10k_df_group.csv\n","Parsed document: /work/data/1993_10k.txt, saved as: /work/data/1993_10k_df_group.csv\n"]}],"source":["# Your list of document paths\n","\n","document_paths = [\n","    \"/work/data/2000_10k.txt\",\n","    \"/work/data/1999_10k.txt\",\n","    \"/work/data/1998_10k.txt\",\n","    \"/work/data/1997_10k.txt\",\n","    \"/work/data/1996_10k.txt\",\n","    \"/work/data/1995_10k.txt\",\n","    \"/work/data/1994_10k.txt\",\n","    \"/work/data/1993_10k.txt\",\n","]\n","\n","# Save to output directory\n","output_dir = \"/work/data/\"  # Desired output directory\n","process_fin_documents(document_paths, output_dir)"]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"93d502266cfe4e069586f00daee42a73","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":165,"execution_start":1718210297631,"id":"YHNHCxI3OpSq","source_hash":"c7e770aa"},"outputs":[],"source":["def load_and_process_fin_documents(document_paths):\n","    \"\"\"\n","    This function filters the 10K csvs to remove sections that don't appear useful and concats them to a new DataFrame.\n","    \"\"\"\n","\n","    dfs = []\n","\n","    for path in document_paths:\n","\n","        df = pd.read_csv(path)\n","\n","        # Define GroupName starting words for exclusion\n","\n","        words_to_exclude = [\n","            \"CONSENT OF INDEPENDENT PUBLIC ACCOUNTANTS\",\n","            \"POWER OF ATTORNEY\",\n","        ]\n","\n","        # Convert df to all strings since partition seems to have added mixed types\n","\n","        df[\"GroupName\"] = df[\"GroupName\"].astype(str)\n","\n","        df[\"grouped_text\"] = df[\"grouped_text\"].astype(str)\n","\n","        # Use the .str.startswith() method with the OR operator to create a boolean mask\n","\n","        mask = df[\"GroupName\"].apply(\n","            lambda x: not any(x.startswith(word) for word in words_to_exclude)\n","        )\n","\n","        # Apply mask to filter DataFrame\n","\n","        df_filtered = df.loc[mask]\n","\n","        # Extract first four characters of the filename (excluding path)\n","\n","        filename = os.path.basename(path).split(\".\")[0][:4]\n","\n","        # Create string to prepend to GroupName\n","\n","        prepend_str = f\"{filename} Annual Report: \"\n","\n","        # Add prepend string to GroupName column\n","\n","        df_filtered.loc[:, \"GroupName\"] = prepend_str + df_filtered[\"GroupName\"]\n","\n","        # Append to list of dataframes\n","\n","        dfs.append(df_filtered)\n","\n","    # Concat all dataframes\n","\n","    fin_combined_df = pd.concat(dfs, ignore_index=True)\n","\n","    return fin_combined_df"]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"b1393efe0b764751b582a9de6aac7b8c","colab":{"base_uri":"https://localhost:8080/"},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1610,"execution_start":1718210297635,"id":"xioDZSvH7L9B","outputId":"6c9088e4-50c3-4c73-8e82-dd93620d295c","source_hash":"ef9bf771"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           GroupName  \\\n","0       2000 Annual Report: Commission File Number 1   \n","1                        2000 Annual Report: GENERAL   \n","2              2000 Annual Report: BUSINESS SEGMENTS   \n","3  2000 Annual Report: TRANSPORTATION AND DISTRIB...   \n","4  2000 Annual Report: Interstate Transmission of...   \n","\n","                                        grouped_text  \n","0  ENRON CORP. (Exact name of registrant as speci...  \n","1  Headquartered in Houston, Texas, Enron Corp., ...  \n","2  Enron has divided its operations into the foll...  \n","3  Enron's Transportation and Distribution busine...  \n","4  Enron and its subsidiaries operate domestic in...  \n","                                        GroupName  \\\n","842        1993 Annual Report: Date: July 1, 1993   \n","843                1993 Annual Report: Exhibit 12   \n","844  1993 Annual Report: ENRON CORP. SUBSIDIARIES   \n","845                1993 Annual Report: Gentlemen:   \n","846                1993 Annual Report: Gentlemen:   \n","\n","                                          grouped_text  \n","842  Primary earnings per share of common stock Inc...  \n","843  Ratio of earnings to fixed charges        1.98...  \n","844  Pipeline Corporation (Delaware) Enron Gas Serv...  \n","845  We hereby consent to the references to our fir...  \n","846  Pursuant to your request, we have prepared est...  \n"]}],"source":["document_paths = [\n","    \"/work/data/2000_10k_df_group.csv\",\n","    \"/work/data/1999_10k_df_group.csv\",\n","    \"/work/data/1998_10k_df_group.csv\",\n","    \"/work/data/1997_10k_df_group.csv\",\n","    \"/work/data/1996_10k_df_group.csv\",\n","    \"/work/data/1995_10k_df_group.csv\",\n","    \"/work/data/1994_10k_df_group.csv\",\n","    \"/work/data/1993_10k_df_group.csv\",\n","]\n","fin_combined_df = load_and_process_fin_documents(document_paths)\n","print(fin_combined_df.head())\n","print(fin_combined_df.tail())"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"b27b5bed7f6245c78ef48825e4583937","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":188,"execution_start":1718210299249,"id":"9p5v5EyvMLus","source_hash":"699d9127"},"outputs":[],"source":["fin_combined_df.to_csv(\"/work/data/fin_combined_df.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["Now the SEC Complaint document ingestion, using the rules-based extraction and parsing of the data from Milestone 1."]},{"cell_type":"code","execution_count":27,"metadata":{"cell_id":"8e9dfeb138e44d289afef1e25a6481e2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":85,"execution_start":1718210299443,"id":"HXwfQI7tNNeb","source_hash":"15152562"},"outputs":[],"source":["def load_and_process_case_documents(document_paths):\n","    \"\"\"\n","    This function filters the SEC case doc csvs to remove sections that don't appear useful and concats them to a new DataFrame.\n","    \"\"\"\n","\n","    dfs = []\n","\n","    for path in document_paths:\n","\n","        df = pd.read_csv(path)\n","\n","        # Define GroupName starting words for exclusion\n","\n","        words_to_exclude = [\"Violation\", \"Violations\", \"Aiding\"]\n","\n","        # Use .str.startswith() method with OR operator to create boolean mask\n","\n","        mask = df[\"GroupName\"].apply(\n","            lambda x: not any(x.startswith(word) for word in words_to_exclude)\n","        )\n","\n","        # Apply mask to filter\n","\n","        df_filtered = df[mask]\n","\n","        # Append to list of dataframes\n","\n","        dfs.append(df_filtered)\n","\n","    # Concat all dataframes\n","\n","    case_combined_df = pd.concat(dfs, ignore_index=True)\n","\n","    return case_combined_df"]},{"cell_type":"code","execution_count":28,"metadata":{"cell_id":"e053d108a1614c289739403c80912f9e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":637,"execution_start":1718210299451,"source_hash":"c4bb8247"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           GroupName  \\\n","0  The Objectives And Roots Of The Scheme To Defraud   \n","1  Use of Special Purpose Entities and LJM Partne...   \n","2                        Creation of LJM Partnership   \n","3                                    \"Raptor\" Hedges   \n","4  Manufacturing Earnings and Concealing Debt thr...   \n","\n","                                        grouped_text  \n","0  The objectives of the scheme to defraud carrie...  \n","1  As part of the scheme to defraud, Skilling, Ca...  \n","2  In June 1999, Skilling, Causey, and others sou...  \n","3  Beginning in the spring of 2000, Enron and LJM...  \n","4  In addition to the fraudulent Raptor hedging d...  \n","                                            GroupName  \\\n","65  Concealment of Uncollectible Receivables Owed ...   \n","66  Concealment of EES Failures by Manipulating Re...   \n","67          Fraudulent Valuation of \"Merchant\" Assets   \n","68  Other Manipulative Devices Used in Enron Whole...   \n","69                         Delainey's Insider Trading   \n","\n","                                         grouped_text  \n","65  Enron also used reserves to conceal huge recei...  \n","66  In the first quarter of 2001, new EES managers...  \n","67  Enron's ENA business unit managed a large \"mer...  \n","68  Enron employed other devices to fraudulently m...  \n","69  Delainey knew of the scheme described in parag...  \n"]}],"source":["document_paths = [\n","    \"/work/data/18776_df_group.csv\",\n","    \"/work/data/20058_df_group.csv\",\n","    \"/work/data/20441_df_group.csv\",\n","    \"/work/data/18435_df_group.csv\",\n","]\n","case_combined_df = load_and_process_case_documents(document_paths)\n","print(case_combined_df.head())\n","print(case_combined_df.tail())"]},{"cell_type":"code","execution_count":29,"metadata":{"cell_id":"6f6b089ff497457f9a90b7c67519ee51","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":235,"execution_start":1718210300094,"source_hash":"5e701b5b"},"outputs":[],"source":["case_combined_df.to_csv(\"/work/data/case_combined_df.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"cell_id":"254ee81df5004526b305280cd84c9055","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Now concat the dataframes."]},{"cell_type":"code","execution_count":30,"metadata":{"cell_id":"505fc6d5091849a0b1254f7ec6187270","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":104,"execution_start":1718210377287,"source_hash":"4adef189"},"outputs":[],"source":["def load_and_concatenate_dfs(path1, path2, path3, output_path):\n","    \"\"\"\n","    Loads dataframes from CSV and pickle files, concatenates them, and saves the result.\n","\n","    Args:\n","        path1 (str): Path to the first CSV file.\n","        path2 (str): Path to the second CSV file.\n","        path3 (str): Path to the pickle file.\n","        output_path (str): Path to save the concatenated dataframe.\n","\n","    Returns:\n","        pandas.DataFrame: The concatenated dataframe.\n","    \"\"\"\n","\n","    # Load CSV dataframes (with explicit UTF-8 encoding)\n","    df1 = pd.read_csv(path1, encoding=\"utf-8\")\n","    df2 = pd.read_csv(path2, encoding=\"utf-8\")\n","\n","    # Load pickle dataframe\n","    with open(path3, \"rb\") as f:\n","        df3 = pd.read_pickle(f)  # Use pd.read_pickle() for pickle files\n","\n","    # Ensure all column values are strings\n","    df1 = df1.astype(str)\n","    df2 = df2.astype(str)\n","    df3 = df3.astype(str)  # If necessary, convert df3 columns as well\n","\n","    # Concat dataframes vertically\n","    combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n","\n","    # Save concatenated dataframe as CSV and pickle\n","    combined_df.to_csv(output_path, index=False)\n","\n","    # with open('/work/data/final_combined.pkl', 'wb') as f:\n","    with open(output_path, \"wb\") as f:\n","        pickle.dump(combined_df, f)\n","\n","    return combined_df"]},{"cell_type":"code","execution_count":31,"metadata":{"cell_id":"1da6b410b7cb4e3ebaf9e563cda9ef48","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1304,"execution_start":1718210385550,"source_hash":"498c4503"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframe saved to /work/data/final_combined_df.csv\n"]}],"source":["# Define the paths\n","path1 = \"/work/data/case_combined_df.csv\"\n","path2 = \"/work/data/fin_combined_df.csv\"\n","path3 = \"/work/data/EnronGoodBad.pkl\"\n","output_path = \"/work/data/final_combined_df.csv\"\n","\n","# Execute function\n","final_df = load_and_concatenate_dfs(path1, path2, path3, output_path)\n","print(f\"Dataframe saved to {output_path}\")\n","\n","\n","# Save concatenated dataframe to specified output path as pickle\n","with open(\"/work/data/final_combined_df.pkl\", \"wb\") as f:\n","    pickle.dump(final_df, f)"]},{"cell_type":"markdown","metadata":{},"source":["final_combined_df.pkl will be used in the next RAG notebook for the augmented retrieval."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"07cb595797ea4468b5dad3d835986cd2","deepnote_cell_type":"code"},"outputs":[],"source":[]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"3d646c3fcacc4cb787743d013fcf16f5","language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
